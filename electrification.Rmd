---
title: "R Notebook"
output: Comercial Building Working Group TG2 Scaling Electrificaiton  
---


```{r, echo="FALSE", include=FALSE, message=FALSE}

###############################################################################
#                             Set up environment                                #
###############################################################################

my_packages <- c("tidyverse", "lubridate", "fable", "tsibbledata", "ggplot2", "forecast", "tseries", "rio", "zoo", "readxl", "tsibbledata") 
                 
lapply(my_packages, require, character.only = TRUE)

#Choose one working site:
place <- "Home"
# place <-  "work"

#Set proper working Dir 
if (place == "Home"){setwd("C:/Users/paulr/Documents/R/cbwg")} else
{setwd("C:/Users/prode/Documents/R/cbwg")}

# Check for data directory and if one is not present then make it
if (!file.exists("data")) {
  dir.create("data")}

rm(place, my_packages )
  
```


The goal with this data project is to use the LL84 benchmarking to identify the thermal potential in each of the building typologies. 
1. Identify the kBTU's of gas, oil and district steam used (thermal fuel) in the commercial stock.
2. Allocate the thermal fuel among building typologies. 
3. Show ranking of typologies by sf and thermal fule volumn. 
3. Show steam in manhattan as a breakout. 



```{r, echo="FALSE", include=FALSE, message=FALSE}

# Read in LL84 data for 2017, change feature names, and zero out NA's


L84_2017 <- read_excel("data/2017_nyc_benchmarking.xlsx", sheet = "Information and Metrics", col_names = TRUE, na = "NA"  ) %>%   mutate(Year = 2017) %>% data.frame()%>% select(Year, `BBL...10.digits`, NYC.Building.Identification.Number..BIN.,
Largest.Property.Use.Type, Self.Reported.Gross.Floor.Area..ft². , Address.1..self.reported., Borough, Year.Built, Natural.Gas.Use..kBtu., Electricity.Use...Grid.Purchase..kBtu., Fuel.Oil..1.Use..kBtu., Fuel.Oil..2.Use..kBtu. , Fuel.Oil..4.Use..kBtu. , Fuel.Oil..5...6.Use..kBtu. ) %>% unique() %>% mutate("count" = 1) %>% filter(Self.Reported.Gross.Floor.Area..ft². >= 50000, Self.Reported.Gross.Floor.Area..ft². < 9000000) 
L84_2017[is.na(L84_2017)] <- 0
L84_2017 <- L84_2017 %>% mutate("total.thermal" = (Natural.Gas.Use..kBtu. + Fuel.Oil..1.Use..kBtu. + Fuel.Oil..2.Use..kBtu. + Fuel.Oil..4.Use..kBtu. + Fuel.Oil..5...6.Use..kBtu.))
L84_2017$Largest.Property.Use.Type <- as.factor(L84_2017$Largest.Property.Use.Type)

colnames(L84_2017) <- c("Year", "BBL", "BIN", "Catagory", "GSF", "Address", "Borough", "Built", "NGas (kBtu)", "Electric (kBtu)" , "1 Oil (kBth)", "2 Oil (kBth)", "4 Oil (kBth)", "6 Oil (kBth)","count", "total.thermal")

#Identify where bulk of sf is by GSF ranges 50,000 up to over 500k.
L84_2017$bins <- cut(L84_2017$GSF, breaks = c(0,50e3, 100e3, 500e3, 1000e3, 9000e3))
GSF_bins <- L84_2017 %>%
  group_by(bins) %>%
  summarise( GSF = sum(GSF), Num.Buildings = sum(count), Thermal = sum(total.thermal), `Electric` = sum(`Electric (kBtu)`)) 


summary(L84_2017)

# Remove outliers for gsf NGas and Electric Usage
take.out <- 10
L84_2017 %>% arrange(GSF) -> L84_2017
L84_2017[take.out:(nrow(L84_2017)-take.out),] -> L84_2017
L84_2017 %>% arrange(`NGas (kBtu)`) -> L84_2017
remove(take.out)

ggplot(data = GSF_bins, aes( x = bins, y = GSF )) +
  geom_bar(stat = "identity") 

L84_2017.sum.catagory <- L84_2017 %>% filter(Catagory == "Office") %>% group_by(Catagory) %>% summarise(GSF = sum(GSF), NGas = sum(`NGas (kBtu)`), Elect = sum(`Electric (kBtu)`))


# Load in LL84 Data for 2016

```

 


```{r, echo="FALSE", include=FALSE, message=FALSE}

ggplot(L84_2017, aes(Catagory)) +
  geom_bar() +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```











```{r, echo=FALSE}
#Before making time series be sure the order is right and make a clean count feature. 
E <- E[order(E[,"Date"]),]
E_ts = ts(E[, c("kWh")])
E$clean_count = tsclean(E$'kWh')
plot(E_ts)


#Running the decompose function with both additive and Multiplicative. With additive being Ts = Seasonal + Trend + Random, and multiplicative being Ts = Seasonal x trend x random. 



#Read in the steam data. 
S <- read_csv("C:/Users/paulr/Documents/R/EnergyForecasting/data/MACH Energy - Data (S).csv", skip = 0, na = "Not Available") 

#Before making time series be sure the order is right and make a clean count feature. 
S <- S[order(E[,"Date"]),]
S_ts = ts(S[, c("Value")])
S$clean_count = tsclean(S$'kWh')
plot(S_ts)


```

Model specifications and estimation.
ETS(tsibble, formulam ...)
ARIMA(tsibble, formula, ...)
TSLM(tsibble, formula, ...)
and many more. 

Model specification with formulas. 
Exmaple: transformation(y) ~ trend() + season(period="day") + X
Like the lk() function rules. 

LHS: Responce
Defines the data's responce variable
Specificaiton of transformations (with automatic back transformation)

RHS: Specials
Model specific specia functions
Exogenous regressors 


Plotting temperutere on time and demand on time and stacking to show visually relationships is recomended. 

tsbl_elec <- tsibbledata::vic_elec %>% filter(Date < ymd("2014-03-01"))
fbl_elec_fit <- tsbl_elec %>%
  model(ARIMA(Demand ~ WorkDay + Temperature + I(Temperature^2),
  pdq(1,0,1) + PDQ(1,1,1, period = "day")))

# # A mable: 1 model
# model
# <model>
# 1LM w/ ARIMA(1,0,1)(1,1,1)[48] erros 


Extract model information using tidy funcitons using the broom package
augment(mable, ....)
tidy(mable, ...)
glance(mable, ...)
components(mable, ...)

Intresting plot can be made using these.
components(fbl_cafe_fit) %>%
  gather(component, value, level, slope, season) %>%
  ggplot(aes(x = Month, y = value)) +
  geom_line() +
  facet_grid(vrs(component), scales = "free_y")
  

Forcast future values
forcast(mable, new_data, h, ...)
Example: fit_cafe_fc <- fbl_cafe_fit %>% forcast(h=24)
fbl_cafe_fc %>% autoplot(vic_cafe)


Electric Demand example
tsbl_elec_new <- tsibbledata::elecdemand %>% 
  filter(index >= ymd("2014-03-01"), index < ymd("2014-03-14"))
fbl_elec_fc <- fbl_elec_fit %>%
  forecast(tsbl_elec_new)

fbl_elec_fc %>% autoplot(tsbl_elec)


  
```{r}
tsbl_elec <- tsibbledata::vic_elec %>% filter(Date < ymd("2014-03-01")) %>% mutate("WorkDay" = wday(Date) )


fbl_elec_fit <- tsbl_elec %>%
  model(arima = ARIMA(Demand ~ WorkDay + Temperature + I(Temperature^2)))
        #, pdq(1,0,1) + PDQ(1,1,1, period = "day"))



tsbl_elec_new <- tsibbledata::vic_elec %>% 
  filter(Date >= ymd("2014-03-01"), Date < ymd("2014-03-14"))


fbl_elec_fc <- fbl_elec_fit %>%
  forecast(tsbl_elec_new)

view(vic_elec)
view(tsbl_elec_new)
```


Accurach evaluation to compare models 
accuracy(mable, measures, ...)
accuracy(fable, new_data, measures, ...)

fbl_cafe_fit %>% accuracy()

Or

fbl_elec_fc %>% accuracy(tsbl_elec_new)


This makes 152 models. 
fbl_cafe_fit 
retail_ets <- tsibbledata::ausretail %>
  ETS(Turnover)


Now use 

Extract(broom::augment)
augment(retail_ets) 

Extract(broom::tidy)
tidy(retail_ets)

Extract(broom::glance)
glance(retail_ets)

Extract(fable::components)
components(retail_ets)

Example:
components(retail_ets) %>%
  filter(Industry == "Cafes, restaurants and takeaway food services") %>%
  ggplot(aes(x = Month, y = level, color = State)) +
  geom_line()
  

Forecast
retail_fc <- retail_ets %>% forecast(h=24)

Evaluate 
retail_ets %>% accuracy()
measures? to check difference in accuracy maybe? 
```{r}

```

Forecasting with Fasster showing electric 

fasster_fit <- tsbl_elec %>%
  fasster(log(Demand) ~ WorkDay %S% (trig(48,16) + poly(1)) + Temperture + I(Temperture^2)) 
  
using exoginest regressors for workday and temperture.

fasster_fc <- fasster_fit %>% 
  forecast(tsbl_elec_new) 
  
  fasster_fc %>% 
    autoplot(tsbl_elec)
    
```{r}


```


Coming Simulate() future paths 

set.seed(20181108)
vic_cafe_sim <- fbl_cafe_fit %>%
  simulate( h = 24, times = 5)
  
  
vic_cafe %>% 
  filter(year(Month) >= 2021) %>% 
  autoplot(Turnover) + 
  geom_line(aes(y = .sim, group = .rep), alpha = 0.3, data = vic_cafe_sim)
  
  
Feture: interpolate() missing values

tsibbledata::olympic_running 

fit a model then interpolate.

olympic_complete <- olympic_running %>% 
  TSLM(Time ~ trend()) %>%
  interpolate(olympic_running) 
  
  refit() allows a model to be applied to a new dataset
  stream() allows a model to be extended using new data
  
  stream is good when you have a model and get additional time series data you just add it and the modeling is redone. 
  
  fasster_stream <- fasster_fit %>% stream(tsbl_elec_new) 
  
Decomposition forecasting
library(tsibblestats)
cafe_dcmp <- vic_cafe %>% STL(log(Turnover)) 


Interval and distribution accuracy
fbl_elec_fc %>% 
accuracy(
  new_data = tsbl_elec_new,
  measures = list(winkler = winkler_score, percentile = perentiale_score)
  
  
)